{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28b7ca8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dataset overview\n",
    "- users.csv: Customer information\n",
    "- orders.csv: Transaction history\n",
    "- subscriptions.csv: Subscription data\n",
    "- preferences.csv: Customer taste preferences\n",
    "- events.scv: User behavioral events (page views, cart actions)\n",
    "- voucher_application: Voucher usage data\n",
    "- user_references.csv: Referral tracking\n",
    "- products.csv: Product catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48278774",
   "metadata": {},
   "source": [
    "## Key Question\n",
    "- Understand what are different groups among current customer base. \n",
    "- How do they differ? What are their needs? \n",
    "- What are the implications for the marketing strategy in terms of attracting similar new customers and/or retaining current customers?\n",
    "- How can these insights be used to recommend QLab Tea’s next action for user growth?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642eb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "if FILTER_WARNINGS:\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.set_loglevel('warning')\n",
    "\n",
    "\n",
    "logging.info(\"Setup completed successfully\")\n",
    "logging.debug(\"Using ENV variables:\")\n",
    "logging.debug(f\"FILTER_WARNINGS: {FILTER_WARNINGS}\")\n",
    "logging.warning(\"This is a warning message.\")\n",
    "logging.error(\"This is an error message.\")\n",
    "logging.critical(\"This is a critical message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Loading datasets\")\n",
    "\n",
    "try:\n",
    "    users_df = pd.read_csv(\"./original_data/users.csv\")\n",
    "    orders_df = pd.read_csv(\"./original_data/orders.csv\")\n",
    "    subscriptions_df = pd.read_csv(\"./original_data/subscriptions.csv\")\n",
    "    products_df = pd.read_csv(\"./original_data/products.csv\")\n",
    "\n",
    "    preferences_df = pd.read_csv(\"./original_data/preferences.csv\")\n",
    "    events_df = pd.read_csv(\"./original_data/events.csv\")\n",
    "    voucher_applications_df = pd.read_csv(\"./original_data/voucher_applications.csv\")\n",
    "    user_references_df = pd.read_csv(\"./original_data/user_references.csv\")\n",
    "\n",
    "    logging.info(f\"Users: {len(users_df):,} rows\")\n",
    "    logging.info(f\"Orders: {len(orders_df):,} rows\")\n",
    "    logging.info(f\"Subscriptions: {len(subscriptions_df):,} rows\")\n",
    "    logging.info(f\"Products: {len(products_df):,} rows\")\n",
    "    logging.info(f\"Preferences: {len(preferences_df):,} rows\")\n",
    "    logging.info(f\"Events: {len(events_df):,} rows\")\n",
    "    logging.info(f\"Voucher Application: {len(voucher_applications_df):,} rows\")\n",
    "    logging.info(f\"User References: {len(user_references_df):,} rows\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Error csv not found: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Load csv error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8568203",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(users_df.head())\n",
    "display(orders_df.head())\n",
    "display(events_df.head())\n",
    "display(subscriptions_df.head())\n",
    "logging.debug(users_df.groupby(\"country\").count())\n",
    "logging.debug(subscriptions_df.groupby(\"currency\").count())\n",
    "logging.debug(orders_df.groupby(\"currency\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d0ee2",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a63d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = {\n",
    "    \"Users\": users_df.isnull().sum(),\n",
    "    \"Orders\": orders_df.isnull().sum(),\n",
    "    \"Subscriptions\": subscriptions_df.isnull().sum(),\n",
    "    \"Products\": products_df.isnull().sum(),\n",
    "    \"Preferences\": preferences_df.isnull().sum(),\n",
    "    \"Events\": events_df.isnull().sum(),\n",
    "    \"Voucher Application\": voucher_applications_df.isnull().sum(),\n",
    "    \"User References\": user_references_df.isnull().sum(),\n",
    "}\n",
    "\n",
    "for k, v in missing_values.items():\n",
    "    logging.debug(f\"{k}\")\n",
    "    logging.debug(f\"{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_clean_df = users_df.copy()\n",
    "del users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2700f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rate = {\n",
    "    \"SGD\": 1.0,\n",
    "    \"HKD\": 0.17,\n",
    "    \"MYR\": 0.31,\n",
    "}\n",
    "\n",
    "\n",
    "orders_clean_df = orders_df.copy()\n",
    "# Only keep completed orders\n",
    "orders_clean_df = orders_clean_df[orders_clean_df[\"status\"].isin([\"Shipped\", \"Delivered\"])]\n",
    "# Remove <=0 order totals\n",
    "orders_clean_df = orders_clean_df[orders_clean_df[\"total_incl_tax\"]>0]\n",
    "# Fix dates\n",
    "orders_clean_df[\"date_placed\"] = pd.to_datetime(orders_clean_df[\"date_placed\"], errors=\"coerce\")\n",
    "orders_clean_df = orders_clean_df.dropna(subset=[\"date_placed\", \"user_id\"])\n",
    "\n",
    "logging.debug(f\"Cleaned orders: {len(orders_clean_df):,} rows\")\n",
    "logging.debug(f\"Date range: {orders_clean_df['date_placed'].min()} - {orders_clean_df['date_placed'].max()}\")\n",
    "logging.debug(f\"Total revenue: {orders_clean_df['total_incl_tax'].sum():.3f}\")\n",
    "\n",
    "def parse_order_items(order_items_json):\n",
    "    try:\n",
    "        items = json.loads(order_items_json)\n",
    "        num_items = sum(int(item.get(\"quantity\", 1)) for item in items)\n",
    "        product_ids = [item.get(\"product_id\") for item in items]\n",
    "        return pd.Series({\n",
    "            \"num_items\": num_items,\n",
    "            \"product_ids\": product_ids\n",
    "        })\n",
    "    except:\n",
    "        return pd.Series({\"num_items\": 1, \"product_ids\": []})\n",
    "    \n",
    "order_items_parsed = orders_clean_df['order_items'].apply(parse_order_items)\n",
    "orders_clean_df = pd.concat([orders_clean_df, order_items_parsed], axis=1)\n",
    "\n",
    "orders_clean_df[\"total_incl_tax_sgd\"] = (\n",
    "    orders_clean_df[\"total_incl_tax\"]\n",
    "    * orders_clean_df[\"currency\"].map(conversion_rate)\n",
    ")\n",
    "\n",
    "logging.debug(\"Order items parsing done\")\n",
    "logging.info(f\"Average item per order: {orders_clean_df['num_items'].mean():.2f}\")\n",
    "logging.debug(orders_clean_df[\"order_items\"][0])\n",
    "del orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptions_clean_df = subscriptions_df.copy()\n",
    "subscriptions_clean_df = subscriptions_clean_df.dropna(subset = [\"user_id\"])\n",
    "date_cols = [\"last_order\", \"next_order\", \"created\", \"updated\"]\n",
    "for col in date_cols:\n",
    "    subscriptions_clean_df[col] = pd.to_datetime(subscriptions_clean_df[col], errors=\"coerce\")\n",
    "del subscriptions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_clean_df = products_df.copy()\n",
    "del products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfeb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences_clean_df = preferences_df.copy()\n",
    "preferences_clean_df = preferences_df.dropna(subset=[\"user_id\"])\n",
    "del preferences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_clean_df = events_df.copy()\n",
    "events_clean_df = events_clean_df.dropna(subset = [\"user_id\", \"timestamp\"])\n",
    "events_clean_df[\"timestamp\"] = pd.to_datetime(events_clean_df[\"timestamp\"], errors=\"coerce\")\n",
    "del events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "voucher_applications_clean_df = voucher_applications_df.copy()\n",
    "voucher_applications_clean_df = voucher_applications_clean_df.dropna(subset=[\"user_id\"])\n",
    "del voucher_applications_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_references_clean_df = user_references_df.copy()\n",
    "del user_references_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5eb760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing_Count': df.isnull().sum(),\n",
    "        'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "    }).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "    logging.info(f\"\\n{missing_summary[missing_summary['Missing_Count'] > 0]}\\n\")\n",
    "\n",
    "# Recheck to see how we did for data cleaning\n",
    "cleaned_dfs = {\n",
    "    \"Users\": users_clean_df,\n",
    "    \"Orders\": orders_clean_df,\n",
    "    \"Subscriptions\": subscriptions_clean_df,\n",
    "    \"Products\": products_clean_df,\n",
    "    \"Preferences\": preferences_clean_df,\n",
    "    \"Events\": events_clean_df,\n",
    "    \"Voucher Application\": voucher_applications_clean_df,\n",
    "    \"User References\": user_references_clean_df,\n",
    "}\n",
    "\n",
    "for k, v in cleaned_dfs.items():\n",
    "    logging.info(f\"{k}\")\n",
    "    check_missing_values(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69598ca4",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2d1eb",
   "metadata": {},
   "source": [
    "### 7 types of features\n",
    "1. RFM (Recency, Frequency, Monetary)\n",
    "2. Behavioral (purchase patterns)\n",
    "3. Product Category\n",
    "4. Subscription\n",
    "5. Engagement (from events)\n",
    "6. Promotion (voucher usage)\n",
    "7. Social (referrals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb9bd1",
   "metadata": {},
   "source": [
    "#### RFM\n",
    "- recency_days = (reference_date - max(order_date)) for each customer\n",
    "- frequency_orders = count of orders per customer\n",
    "- monetary_total = sum of order totals amount per customer\n",
    "- avg_order_value = monetary_total/frequency_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e40c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newest_date = orders_clean_df[\"date_placed\"].max()\n",
    "oldest_date = orders_clean_df[\"date_placed\"].min()\n",
    "\n",
    "logging.debug(f\"Reference date: {newest_date}\")\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def sigmoid_series(series, steepness):\n",
    "#     series_range = series.max() - series.min() or pd.Timedelta(days=1)\n",
    "#     return sigmoid( \n",
    "#         steepness * (\n",
    "#         (\n",
    "#             (series - series.min()) / series_range\n",
    "#         ) \n",
    "#         - 0.5)\n",
    "#     )\n",
    "#\n",
    "# orders_clean_df[\"weighted_total_incl_tax_sgd\"] = (\n",
    "#     orders_clean_df[\"total_incl_tax_sgd\"]\n",
    "#     * sigmoid_series(orders_clean_df[\"date_placed\"], 12)\n",
    "# )\n",
    "\n",
    "def arbitrary_date_weights(dates):\n",
    "    year = dates.dt.year\n",
    "    return np.select(\n",
    "        [year <= 2019, year <= 2020, year <= 2021],\n",
    "        [0.2, 0.5, 0.8],\n",
    "        default=1.0,\n",
    "    )\n",
    "\n",
    "orders_clean_df[\"monetary_weight\"] = arbitrary_date_weights(orders_clean_df[\"date_placed\"])\n",
    "\n",
    "orders_clean_df[\"weighted_total_incl_tax_sgd\"] = (\n",
    "    orders_clean_df[\"total_incl_tax_sgd\"] * orders_clean_df[\"monetary_weight\"]\n",
    ")\n",
    "\n",
    "rfm_features = orders_clean_df.groupby(\"user_id\").agg(\n",
    "    {\n",
    "        \"date_placed\": [\n",
    "            lambda x: (newest_date - x.max()).days,                     # Recency\n",
    "            lambda x: len(x) / max((x.max() - x.min()).days, 1)         # Frequency\n",
    "        ],\n",
    "        \"weighted_total_incl_tax_sgd\": \"sum\" # Monetary\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "rfm_features.columns = [\"user_id\", \"recency_days\", \"frequency_orders\", \"monetary_total\"]\n",
    "\n",
    "order_counts = orders_clean_df.groupby(\"user_id\")[\"id\"].count().rename(\"order_count\")\n",
    "rfm_features = rfm_features.join(order_counts, on=\"user_id\")\n",
    "\n",
    "rfm_features[\"avg_order_value\"] = (\n",
    "    rfm_features[\"monetary_total\"] / rfm_features[\"order_count\"].clip(lower=1)\n",
    ")\n",
    "\n",
    "rfm_features[\"avg_order_value\"] = rfm_features[\"monetary_total\"]/rfm_features[\"frequency_orders\"]\n",
    "\n",
    "\n",
    "logging.debug(f\"\\n{rfm_features.describe()}\")\n",
    "display(rfm_features.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6638c",
   "metadata": {},
   "source": [
    "#### Behavioral\n",
    "- avg_items_per_order = Average number of items purchased per order\n",
    "- total_items = Total number of items ever purchased\n",
    "- num_addresses = Number of different shipping addresses (possible gifting or multi location)\n",
    "- primary_currency = Most commonly used currency\n",
    "- order_regularity_std = Standard deviation of days between orders (lower indicate regular purchases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5944e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_features = orders_clean_df.groupby(\"user_id\").agg({\n",
    "    \"num_items\": [\"mean\", \"sum\"], # average number of items per order, total items\n",
    "    \"shipping_postal_code\": \"nunique\", # number of addresses\n",
    "    \"currency\": lambda x: x.mode()[0] if len(x.mode()) > 0 else \"SGD\", # Most common currency type\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "behavioral_features.columns = [\"user_id\",\n",
    "                               \"avg_items_per_order\",\n",
    "                               \"total_items\",\n",
    "                               \"num_addresses\",\n",
    "                               \"primary_currency_behavioral\",\n",
    "                               #\"order_regularity_std\"\n",
    "                               ]\n",
    "\n",
    "def calc_order_regularity(user_orders):\n",
    "    if len(user_orders) < 2: return 0\n",
    "    dates = user_orders.sort_values(\"date_placed\")[\"date_placed\"]\n",
    "    days_between = dates.diff().dt.days.dropna()\n",
    "    return days_between.std() if len(days_between) > 0 else 0\n",
    "\n",
    "order_regularity = orders_clean_df.groupby(\"user_id\").apply(calc_order_regularity).reset_index()\n",
    "order_regularity.columns = [\"user_id\", \"order_regularity_std\"]\n",
    "\n",
    "behavioral_features = behavioral_features.merge(order_regularity, on=\"user_id\")\n",
    "\n",
    "logging.debug(f\"\\n{behavioral_features.describe()}\")\n",
    "logging.debug(f\"\\n{behavioral_features.head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35492d02",
   "metadata": {},
   "source": [
    "#### Product Category\n",
    "- tea_pod_pct = Percentage of purchases that are tea pods\n",
    "- tea_bag_pct = Percentage of purchases that are tea bags\n",
    "- merchandise_pct = Percentage of purchases that are merchandise\n",
    "- bundle_pct = Percentage of purchases that are bundles/variety packs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd68239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Might need to check through this again to confirm\n",
    "\n",
    "def categorize_product(title):\n",
    "    title_lower = str(title).lower()\n",
    "\n",
    "    match_strings_pod = (\"teapods\", \"pods\")\n",
    "    match_strings_merch = (\"bag\", \"qlab\")\n",
    "    match_strings_subscription = (\"subscription\",)\n",
    "    match_strings_bundle = (\"bundle\", \"set\", \"pack\", \"variety\")\n",
    "\n",
    "    for keyterm in match_strings_pod:\n",
    "        if keyterm in title_lower:\n",
    "            return \"tea_pod\"\n",
    "    \n",
    "    for keyterm in match_strings_merch:\n",
    "        if keyterm in title_lower:\n",
    "            return \"merchandise\"\n",
    "        \n",
    "    for keyterm in match_strings_subscription:\n",
    "        if keyterm in title_lower:\n",
    "            return \"subscription\"\n",
    "    \n",
    "    for keyterm in match_strings_bundle:\n",
    "        if keyterm in title_lower:\n",
    "            return \"bundle\"\n",
    "        \n",
    "    return \"tea_bag\"\n",
    "\n",
    "products_clean_df[\"category\"] = products_clean_df[\"title\"].apply(categorize_product)\n",
    "logging.debug(products_clean_df[\"category\"].value_counts())\n",
    "\n",
    "def get_product_categories(product_ids_list):\n",
    "    if not isinstance(product_ids_list, list) or len(product_ids_list)==0: return []\n",
    "\n",
    "    categories = []\n",
    "    for pid in product_ids_list:\n",
    "        if pid is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pid_int = int(pid)\n",
    "            matching_products = products_clean_df[products_clean_df[\"product_id\"]==pid_int]\n",
    "            if len(matching_products) > 0:\n",
    "                categories.append(matching_products.iloc[0][\"category\"])\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    return categories\n",
    "\n",
    "\n",
    "product_features_list = []\n",
    "total_customers = len(rfm_features)\n",
    "\n",
    "for idx, user_id in enumerate(rfm_features[\"user_id\"]):\n",
    "    if (idx+1) %1000 == 0:\n",
    "        logging.debug(f\"Processing {idx+1}/{total_customers} customers\")\n",
    "    user_orders = orders_clean_df[orders_clean_df[\"user_id\"] == user_id]\n",
    "\n",
    "    all_product_ids = []\n",
    "    for pids in user_orders[\"product_ids\"]:\n",
    "        if isinstance(pids, list):\n",
    "            all_product_ids.extend(pids)\n",
    "\n",
    "    categories = get_product_categories(all_product_ids)\n",
    "\n",
    "    total = len(categories) if len(categories) > 0 else 1\n",
    "\n",
    "    product_features_list.append({\n",
    "        \"user_id\": user_id,\n",
    "        \"tea_pod_pct\": (categories.count(\"tea_pod\") / total) * 100,\n",
    "        \"tea_bag_pct\": (categories.count(\"tea_bag\") / total) * 100,\n",
    "        \"merchandise_pct\": (categories.count(\"merchandise\") / total) * 100,\n",
    "        \"bundle_pct\": (categories.count(\"bundle\") / total) * 100\n",
    "        }\n",
    "    )\n",
    "\n",
    "product_features = pd.DataFrame(product_features_list)\n",
    "\n",
    "logging.debug(product_features.describe())\n",
    "display(product_features.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15545a4f",
   "metadata": {},
   "source": [
    "#### Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_features = subscriptions_clean_df.copy()\n",
    "\n",
    "subscription_features[\"is_active\"] = subscription_features[\"status\"].str.lower().eq(\"active\").astype(int)\n",
    "subscription_features[\"surprise_flag\"] = subscription_features[\"surprise\"].str.strip().str.lower().eq(\"yes\").astype(int)\n",
    "subscription_features[\"order_gap_days\"] = (subscription_features[\"next_order\"] - subscription_features[\"last_order\"]).dt.days\n",
    "subscription_features[\"subscription_age_days\"] = (pd.Timestamp(\"today\") - subscription_features[\"created\"]).dt.days\n",
    "\n",
    "# --- aggregate per user ---\n",
    "subscription_features = subscription_features.groupby(\"user_id\").agg(\n",
    "    total_subscription_records=(\"product_id\", \"count\"),\n",
    "    unique_products=(\"product_id\", \"nunique\"),\n",
    "    num_shipping_addresses=(\"shipping_postal_code\", \"nunique\"),\n",
    "    num_countries=(\"country\", \"nunique\"),\n",
    "    avg_interval=(\"interval\", \"mean\"),\n",
    "    interval_std=(\"interval\", \"std\"),\n",
    "    active_subscriptions=(\"is_active\", \"sum\"),\n",
    "    active_ratio=(\"is_active\", \"mean\"),\n",
    "    primary_shipping_method=(\"shipping_method\", lambda x: x.mode()[0] if len(x.mode()) else \"unknown\"),\n",
    "    primary_currency_subscription=(\"currency\", lambda x: x.mode()[0] if len(x.mode()) else \"SGD\"),\n",
    "    surprise_ratio=(\"surprise_flag\", \"mean\"),\n",
    "    avg_order_gap_days=(\"order_gap_days\", \"mean\"),\n",
    "    order_gap_std=(\"order_gap_days\", \"std\"),\n",
    "    avg_subscription_age_days=(\"subscription_age_days\", \"mean\"),\n",
    "    first_subscription_created=(\"created\", \"min\"),\n",
    "    latest_subscription_update=(\"updated\", \"max\"),\n",
    ").reset_index()\n",
    "\n",
    "logging.debug(f\"\\n{subscription_features.describe()}\")\n",
    "logging.debug(f\"\\n{subscription_features.head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59528d8",
   "metadata": {},
   "source": [
    "#### Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(events_clean_df.head(10))\n",
    "logging.debug(events_clean_df[\"event\"].unique())\n",
    "\n",
    "events_df = events_clean_df.copy()\n",
    "events_df[\"event_date\"] = events_df[\"timestamp\"].dt.date\n",
    "\n",
    "high_intent_events = {\n",
    "    \"ACTION_VIEW_PRODUCT\",\n",
    "    \"ACTION_ADD_ITEM_TO_CART\",\n",
    "    \"ACTION_VOUCHER_APPLY\",\n",
    "    \"PURCHASED_GIFT_CARD\",\n",
    "}\n",
    "\n",
    "events_df[\"is_high_intent\"] = events_df[\"event\"].isin(high_intent_events).astype(int)\n",
    "\n",
    "last_ts = events_df[\"timestamp\"].max().normalize()\n",
    "\n",
    "engagement_core = events_df.groupby(\"user_id\").agg(\n",
    "    total_events=(\"event\", \"count\"),\n",
    "    unique_event_types=(\"event\", \"nunique\"),\n",
    "    active_days=(\"event_date\", \"nunique\"),\n",
    "    high_intent_events=(\"is_high_intent\", \"sum\"),\n",
    "    last_event_ts=(\"timestamp\", \"max\"),\n",
    ").reset_index()\n",
    "\n",
    "total_sessions = (events_df[events_df[\"event\"]==\"ACTION_LOGIN\"].groupby(\"user_id\").size().reset_index(name=\"total_sessions\"))\n",
    "\n",
    "engagement_core[\"events_per_active_day\"] = (\n",
    "    engagement_core[\"total_events\"] / engagement_core[\"active_days\"].clip(lower=1)\n",
    ")\n",
    "engagement_core[\"high_intent_ratio\"] = (\n",
    "    engagement_core[\"high_intent_events\"] / engagement_core[\"total_events\"].clip(lower=1)\n",
    ")\n",
    "engagement_core[\"days_since_last_event\"] = (\n",
    "    last_ts - engagement_core[\"last_event_ts\"]\n",
    ").dt.days\n",
    "\n",
    "key_event_counts = (\n",
    "    events_df[events_df[\"event\"].isin(high_intent_events)]\n",
    "    .pivot_table(index=\"user_id\", columns=\"event\", values=\"timestamp\", aggfunc=\"count\", fill_value=0)\n",
    "    .add_prefix(\"event_\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "engagement_features = (\n",
    "    engagement_core\n",
    "    .merge(total_sessions, on=\"user_id\", how=\"left\")\n",
    "    .merge(key_event_counts, on=\"user_id\", how=\"left\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "logging.debug(f\"\\n{engagement_features.describe(include='all')}\")\n",
    "logging.debug(f\"\\n{engagement_features.head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106d1d6",
   "metadata": {},
   "source": [
    "#### Promotion (Voucher Usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.info(\"--- Starting Voucher Feature Engineering (Group Part) ---\")\n",
    "\n",
    "try:\n",
    "    vouchers_df = pd.read_csv(\"./original_data/vouchers.csv\")\n",
    "    # 提取所需字段，不修改原始文件\n",
    "    # 假设 'total_discount' 是固定折扣金额，'date_created' 用于计算时效性\n",
    "    vouchers_meta = vouchers_df[['id', 'total_discount', 'date_created']].rename(\n",
    "        columns={'id': 'voucher_id', 'total_discount': 'discount_val'}\n",
    "    )\n",
    "    vouchers_meta['discount_val'] = vouchers_meta['discount_val'].fillna(0)\n",
    "    vouchers_meta['date_created'] = pd.to_datetime(vouchers_meta['date_created'])\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"vouchers.csv not found!\")\n",
    "    raise\n",
    "\n",
    "# 2. 基础关联：Orders + Applications + Vouchers\n",
    "# 严守不改名原则：Orders(id) <-> Applications(order_id)\n",
    "promo_merged = orders_clean_df[['id', 'user_id', 'total_incl_tax']].merge(\n",
    "    voucher_applications_clean_df[['order_id', 'voucher_id', 'created']], \n",
    "    left_on='id', \n",
    "    right_on='order_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 关联 Voucher 详情\n",
    "promo_merged = promo_merged.merge(vouchers_meta, on='voucher_id', how='left')\n",
    "\n",
    "# 3. 计算订单级指标\n",
    "promo_merged['has_voucher'] = promo_merged['voucher_id'].notna().astype(int)\n",
    "promo_merged['savings'] = np.where(promo_merged['has_voucher'] == 1, promo_merged['discount_val'], 0)\n",
    "\n",
    "# 计算 lag_days (领券/用券 滞后天数): 使用时间(created) - 发布时间(date_created)\n",
    "promo_merged['created'] = pd.to_datetime(promo_merged['created'])\n",
    "promo_merged['lag_days'] = (promo_merged['created'] - promo_merged['date_created']).dt.days\n",
    "\n",
    "# 4. 用户级聚合 (User Aggregation)\n",
    "voucher_stats = promo_merged.groupby('user_id').agg(\n",
    "    total_orders=('id', 'count'),\n",
    "    voucher_orders_count=('has_voucher', 'sum'),\n",
    "    total_savings=('savings', 'sum'),\n",
    "    total_revenue=('total_incl_tax', 'sum'),\n",
    "    unique_vouchers_used=('voucher_id', 'nunique'), # 进阶：用过几种不同的券\n",
    "    avg_voucher_lag_days=('lag_days', 'mean')       # 进阶：平均响应天数\n",
    ").reset_index()\n",
    "\n",
    "# 5. 计算比率特征\n",
    "voucher_stats['voucher_usage_rate'] = voucher_stats['voucher_orders_count'] / voucher_stats['total_orders']\n",
    "voucher_stats['discount_savings_ratio'] = voucher_stats.apply(\n",
    "    lambda x: x['total_savings'] / x['total_revenue'] if x['total_revenue'] > 0 else 0, axis=1\n",
    ")\n",
    "\n",
    "# 6. 缺失值处理\n",
    "voucher_stats['avg_voucher_lag_days'] = voucher_stats['avg_voucher_lag_days'].fillna(-1)\n",
    "voucher_stats[['voucher_orders_count', 'total_savings', 'unique_vouchers_used']] = \\\n",
    "    voucher_stats[['voucher_orders_count', 'total_savings', 'unique_vouchers_used']].fillna(0)\n",
    "\n",
    "# 7. 输出最终表 (只保留特征列 + user_id)\n",
    "voucher_features = voucher_stats[[\n",
    "    'user_id', \n",
    "    'voucher_orders_count', \n",
    "    'voucher_usage_rate', \n",
    "    'total_savings', \n",
    "    'discount_savings_ratio',\n",
    "    'unique_vouchers_used',\n",
    "    'avg_voucher_lag_days'\n",
    "]]\n",
    "\n",
    "logging.info(f\"Voucher Part Complete. Shape: {voucher_features.shape}\")\n",
    "display(voucher_features.head())\n",
    "# 1. 统计有多少人真正用过券\n",
    "used_count = len(voucher_features[voucher_features['voucher_orders_count'] > 0])\n",
    "print(f\"用过优惠券的用户数量: {used_count}\")\n",
    "print(f\"占比: {used_count / len(voucher_features):.2%}\")\n",
    "\n",
    "# 2. 展示前 5 个用过券的“真实数据”\n",
    "print(\"\\n--- 用过券的用户示例 (非0数据) ---\")\n",
    "display(voucher_features[voucher_features['voucher_orders_count'] > 0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f0e97",
   "metadata": {},
   "source": [
    "#### Social (Referrals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56442f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.info(\"--- Starting Social Feature Engineering (Group Part) ---\")\n",
    "\n",
    "# 1. 推荐人维度 (Referrer): 统计拉了多少人\n",
    "# 统计 referred_by 出现的次数\n",
    "referrer_stats = user_references_clean_df.groupby('referred_by').size().reset_index(name='num_referred_users')\n",
    "\n",
    "# 2. 被推荐人维度 (Referee): 谁是被拉进来的\n",
    "referee_users_set = set(user_references_clean_df['user_id'].unique())\n",
    "\n",
    "# 3. 进阶时间特征: 注册后多久开始拉人 (Days to First Referral)\n",
    "# 找到每个推荐人的第一次推荐时间\n",
    "first_ref_time = user_references_clean_df.groupby('referred_by')['created'].min().reset_index(name='first_referral_date')\n",
    "first_ref_time['first_referral_date'] = pd.to_datetime(first_ref_time['first_referral_date'])\n",
    "\n",
    "# 4. 合并回用户表 (User Base)\n",
    "# 以 Users 表为基准，因为我们需要 date_signed_up 来计算时间差\n",
    "# 严守不改名原则：Users(id) <-> user_references(referred_by)\n",
    "social_base = users_clean_df[['id', 'date_signed_up']].drop_duplicates()\n",
    "\n",
    "# 合并推荐数量\n",
    "social_base = social_base.merge(\n",
    "    referrer_stats,\n",
    "    left_on='id',\n",
    "    right_on='referred_by',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 合并第一次推荐时间\n",
    "social_base = social_base.merge(\n",
    "    first_ref_time,\n",
    "    left_on='id',\n",
    "    right_on='referred_by',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 5. 计算特征\n",
    "# Feature A: 成功推荐人数 (空值填0)\n",
    "social_base['num_referred_users'] = social_base['num_referred_users'].fillna(0).astype(int)\n",
    "\n",
    "# Feature B: 是否被推荐用户 (0/1)\n",
    "social_base['is_referred_user'] = social_base['id'].apply(lambda x: 1 if x in referee_users_set else 0)\n",
    "\n",
    "# Feature C: 注册到首次推荐的天数差 (Days to First Referral)\n",
    "social_base['date_signed_up'] = pd.to_datetime(social_base['date_signed_up'])\n",
    "social_base['days_to_first_referral'] = (social_base['first_referral_date'] - social_base['date_signed_up']).dt.days\n",
    "\n",
    "# 处理时间差的空值 (没推荐过人 = -1)\n",
    "social_base['days_to_first_referral'] = social_base['days_to_first_referral'].fillna(-1)\n",
    "\n",
    "# 6. 清理列 (删除合并产生的辅助列)\n",
    "cols_to_drop = [c for c in social_base.columns if 'referred_by' in c or 'date_' in c]\n",
    "social_features_clean = social_base.drop(columns=cols_to_drop)\n",
    "\n",
    "# 重命名 id 为 user_id 以便最后统一合并 (可选，如果组长要求保留 id 则不跑这行)\n",
    "social_features = social_features_clean.rename(columns={'id': 'user_id'})\n",
    "\n",
    "logging.info(f\"Social Part Complete. Shape: {social_features.shape}\")\n",
    "display(social_features.head())\n",
    "\n",
    "# 1. 看看有多少“带货王” (成功推荐过别人)\n",
    "koc_df = social_features[social_features['num_referred_users'] > 0]\n",
    "print(f\"成功推荐过他人的用户数: {len(koc_df)}\")\n",
    "\n",
    "# 2. 看看有多少“被安利的人” (通过推荐注册)\n",
    "referee_df = social_features[social_features['is_referred_user'] == 1]\n",
    "print(f\"通过推荐注册的用户数: {len(referee_df)}\")\n",
    "\n",
    "# 3. 展示一下真实的社交数据\n",
    "print(\"\\n--- 社交达人 (KOC) 示例 ---\")\n",
    "display(koc_df.head())\n",
    "\n",
    "print(\"\\n--- 刚注册就拉人的用户 (时间差小) ---\")\n",
    "# 筛选出推荐过人，且时间差不为 -1 的\n",
    "active_referrers = social_features[social_features['days_to_first_referral'] >= 0]\n",
    "display(active_referrers.sort_values('days_to_first_referral').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c42e4c",
   "metadata": {},
   "source": [
    "#### TASTE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c38cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"preferences 让我看看:\")\n",
    "#display(preferences_clean_df.head(10))\n",
    "print(preferences_clean_df.groupby('user_id').size().value_counts())\n",
    "\n",
    "taste_features = preferences_clean_df[['user_id', 'intensity']].drop_duplicates(subset='user_id')\n",
    "print(taste_features.groupby('user_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1390597",
   "metadata": {},
   "source": [
    "### Combine all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = rfm_features.copy()\n",
    "customer_features = customer_features.merge(behavioral_features, on=\"user_id\", how=\"left\")\n",
    "customer_features = customer_features.merge(product_features, on=\"user_id\", how=\"left\")\n",
    "customer_features = customer_features.merge(subscription_features, on=\"user_id\", how=\"left\")\n",
    "customer_features = customer_features.merge(engagement_features, on=\"user_id\", how=\"left\")\n",
    "customer_features = customer_features.merge(voucher_features, on=\"user_id\", how=\"left\")\n",
    "customer_features = customer_features.merge(social_features, on=\"user_id\", how=\"left\")\n",
    "customer_features = customer_features.merge(taste_features, on=\"user_id\", how=\"left\")\n",
    "\n",
    "customer_features[\"intensity\"] = customer_features[\"intensity\"].fillna('unknown')\n",
    "\n",
    "\n",
    "customer_features[\"conversion_rate\"] = customer_features.apply(\n",
    "    lambda row: row[\"frequency_orders\"] / row[\"total_sessions\"] if row[\"total_sessions\"] > 0 else 0,\n",
    "    axis = 1,\n",
    ")\n",
    "\n",
    "fill_zero_cols = [\n",
    "\n",
    "]\n",
    "\n",
    "for col in fill_zero_cols:\n",
    "    if col in customer_features.columns:\n",
    "        customer_features[col] = customer_features[col].fillna(0)\n",
    "\n",
    "max_regularity = customer_features[\"order_regularity_std\"].max()\n",
    "\n",
    "if pd.isna(max_regularity):\n",
    "    max_regularity = 999\n",
    "\n",
    "customer_features[\"order_regularity_std\"] = customer_features[\"order_regularity_std\"].fillna(max_regularity)\n",
    "\n",
    "logging.debug(customer_features.columns.tolist())\n",
    "logging.debug(customer_features.head())\n",
    "logging.debug(customer_features.describe())\n",
    "\n",
    "display(customer_features.head())\n",
    "display(customer_features['intensity'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64a681",
   "metadata": {},
   "source": [
    "#### taste features intensity分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n合并后的 intensity 分布:\")\n",
    "print(customer_features['intensity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d07fcc",
   "metadata": {},
   "source": [
    "### Post combination processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30066abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are values in primary_currency that are NaN, we use the other primary_currency column to fill\n",
    "\n",
    "behavioral = customer_features[\"primary_currency_behavioral\"]\n",
    "subscription = customer_features[\"primary_currency_subscription\"]\n",
    "\n",
    "customer_features[\"primary_currency_behavioral\"] = behavioral.fillna(subscription)\n",
    "customer_features[\"primary_currency_subscription\"] = subscription.fillna(\n",
    "    customer_features[\"primary_currency_behavioral\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2626a6a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2751c0",
   "metadata": {},
   "source": [
    "### Split numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd940dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "num_features = customer_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_features = customer_features.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "logging.info(f\"Numerical features ({len(num_features)}): {num_features}\")\n",
    "logging.debug(f\"Categorical features ({len(cat_features)}): {cat_features}\")\n",
    "\n",
    "# Detailed summary for numerical features\n",
    "logging.debug(customer_features[num_features].describe())\n",
    "\n",
    "# Check for unique values in categorical features\n",
    "for col in cat_features:\n",
    "    logging.debug(f\"{col}: {customer_features[col].nunique()} unique values\")\n",
    "\n",
    "\n",
    "\n",
    "# Special consideration for primary currency mismatch\n",
    "# 1) Boolean flag per row\n",
    "customer_features[\"currency_mismatch\"] = (\n",
    "    customer_features[\"primary_currency_behavioral\"]\n",
    "    != customer_features[\"primary_currency_subscription\"]\n",
    ")\n",
    "\n",
    "# 2) Inspect any mismatches\n",
    "mismatched = customer_features[customer_features[\"currency_mismatch\"]]\n",
    "\n",
    "print(f\"Mismatched customers: {len(mismatched)}\")\n",
    "print(mismatched[[\"user_id\", \"primary_currency_behavioral\", \"primary_currency_subscription\"]].head())\n",
    "print(customer_features.groupby(\"primary_currency_behavioral\").count()[\"user_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1913dd",
   "metadata": {},
   "source": [
    "### Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19334b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values percentage\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': customer_features.isnull().sum(),\n",
    "    'Missing_Percentage': (customer_features.isnull().sum() / len(customer_features)) * 100\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing values pattern\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(customer_features.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Values Pattern')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fb081",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59856f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_summary = {}\n",
    "for col in num_features:\n",
    "    Q1 = customer_features[col].quantile(0.25)\n",
    "    Q3 = customer_features[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = customer_features[(customer_features[col] < lower_bound) | (customer_features[col] > upper_bound)][col]\n",
    "    outlier_percentage = (len(outliers) / len(customer_features)) * 100\n",
    "    outlier_summary[col] = outlier_percentage\n",
    "\n",
    "outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', \n",
    "                                   columns=['Outlier_Percentage'])\n",
    "print(\"Outlier Summary (IQR method):\")\n",
    "print(outlier_df.sort_values('Outlier_Percentage', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2928a",
   "metadata": {},
   "source": [
    "### Distribution of key customer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(10, 15))\n",
    "fig.suptitle(\"Distribution of Key Customer Features\", fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "# Recency\n",
    "axes[0, 0].hist(customer_features[\"recency_days\"], bins=50, edgecolor=\"black\")\n",
    "axes[0, 0].set_title(\"Recency (Days since last order)\")\n",
    "axes[0, 0].set_xlabel(\"Days\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Frequency\n",
    "axes[0, 1].hist(customer_features[\"frequency_orders\"], bins=50, edgecolor=\"black\")\n",
    "axes[0, 1].set_title(\"Frequency (Numbers of Orders)\")\n",
    "axes[0, 1].set_xlabel(\"Number of Orders\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Monetary\n",
    "axes[0, 2].hist(customer_features[\"monetary_total\"], bins=50, edgecolor=\"black\")\n",
    "axes[0, 2].set_title(\"Monetary (Total Spend)\")\n",
    "axes[0, 2].set_xlabel(\"Number Spend ($)\")\n",
    "axes[0, 2].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Subscription status\n",
    "# subscription_counts = customer_features[\"has_subscription\"].value_counts()\n",
    "customer_features[\"has_subscription\"] = (customer_features[\"active_subscriptions\"].fillna(0) > 0).astype(int)\n",
    "subscription_counts = customer_features[\"has_subscription\"].value_counts()\n",
    "axes[1, 0].bar([\"No Subscription\", \"Has Subscription\"], subscription_counts.values)\n",
    "axes[1, 0].set_title(\"Subscription Status Distribution\")\n",
    "axes[1, 0].set_ylabel(\"Number of Customers\")\n",
    "\n",
    "# Engagement\n",
    "axes[1, 1].hist(customer_features[\"total_events\"], bins=50, edgecolor=\"black\")\n",
    "axes[1, 1].set_title(\"Total Engagement Events\")\n",
    "axes[1, 1].set_xlabel(\"Number of Events\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Voucher usage\n",
    "#voucher_counts = (customer_features[\"is_voucher_user\"] > 0).value_counts()\n",
    "voucher_counts = (customer_features[\"voucher_orders_count\"].fillna(0) > 0).value_counts()\n",
    "axes[1, 2].bar([\"No Vouchers\", \"Used Vouchers\"], voucher_counts.values)\n",
    "axes[1, 2].set_title(\"Voucher Usage Distribution\")\n",
    "axes[1, 2].set_ylabel(\"Number of Customers\")\n",
    "\n",
    "# Taste Preferences,   这个地方我改了，注意  注意  注意\n",
    "customer_features['has_preferences'] = customer_features['user_id'].isin(preferences_clean_df['user_id']).astype(int)\n",
    "taste_counts = (customer_features[\"has_preferences\"] >0).value_counts()\n",
    "axes[2, 0].bar([\"No Preferences\", \"Has Preferences\"], taste_counts.values)\n",
    "axes[2, 0].set_title(\"Taste Preferences Set\")\n",
    "axes[2, 0].set_ylabel(\"Number of Customers\")\n",
    "\n",
    "axes[2, 1].axis('off')\n",
    "axes[2, 2].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d513c",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 14))\n",
    "\n",
    "#这里新加了selct_dtypes去取数值类型，因为只能计算数值类型的相关性\n",
    "\n",
    "numeric_features = customer_features.select_dtypes(include=[np.number]).drop(\"user_id\", axis=1, errors=\"ignore\")\n",
    "\n",
    "correlation_matrix = numeric_features.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.0})\n",
    "plt.title(\"Feature Correlation Matrix\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06987b",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_features = customer_features.drop([\"user_id\", \"primary_currency\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "logging.debug(f\"Features for clustering: {len(clustering_features.columns)} features\")\n",
    "logging.debug(f\"Shape: {clustering_features.shape}\")\n",
    "\n",
    "missing_count = clustering_features.isnull().sum().sum()\n",
    "logging.debug(f\"Missing values: {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    logging.warning(\"Found missing values\")\n",
    "    # 这里我也改了，非数据和数据类型不同的处理方式  注意 注意  注意\n",
    "    # 只对数值型列填充中位数\n",
    "    num_cols = clustering_features.select_dtypes(include=[np.number]).columns\n",
    "    cluster_features = clustering_features.copy()\n",
    "    cluster_features[num_cols] = cluster_features[num_cols].fillna(cluster_features[num_cols].median())\n",
    "    # 对非数值型列用众数填充\n",
    "    non_num_cols = clustering_features.select_dtypes(exclude=[np.number]).columns\n",
    "    for col in non_num_cols:\n",
    "        mode_val = clustering_features[col].mode()\n",
    "        if not mode_val.empty:\n",
    "            cluster_features[col] = cluster_features[col].fillna(mode_val[0])\n",
    "        else:\n",
    "            cluster_features[col] = cluster_features[col].fillna(\"unknown\")\n",
    "    logging.warning(\"Missing values filled with median/mode\")\n",
    "else:\n",
    "    cluster_features = clustering_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#注意这里只能是数据类型  所以又改了  注意  注意  注意\n",
    "numeric_cluster_features = cluster_features.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(numeric_cluster_features)\n",
    "\n",
    "logging.debug(\"Clutering Features scaled using StandardScaler\")\n",
    "logging.debug(f\"Scaled features shape: {features_scaled.shape}\")\n",
    "logging.debug(f\"Scaled features mean: {features_scaled.mean():.4f}\") # Should be near 0\n",
    "logging.debug(f\"Scaled features std: {features_scaled.std():.4f}\") # Should be near 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0ebfc",
   "metadata": {},
   "source": [
    "### Testing Different Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "inertias = []\n",
    "silhouette_scores_list = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    logging.debug(f\"Testing k={k}\")\n",
    "\n",
    "    k_means = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "    k_means.fit(features_scaled)\n",
    "\n",
    "    inertias.append(k_means.inertia_)\n",
    "    silhouette_scores_list.append(silhouette_score(features_scaled, k_means.labels_))\n",
    "\n",
    "    logging.info(f\"K={k} | Inertia={inertias[-1]:.3f} | Silhouette: {silhouette_scores_list[-1]:.3f}\")\n",
    "\n",
    "\n",
    "plt, axes = plt.subplots(1, 2, figsize=(14, 9))\n",
    "\n",
    "axes[0].plot(k_range, inertias, \"ro-\", linewidth=2, markersize=3)\n",
    "axes[0].set_xlabel(\"Number of Clusters (k)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Inertia (Within-cluster sum of squares)\", fontsize=12)\n",
    "axes[0].set_title(\"Elbow method\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(k_range, silhouette_scores_list, \"ro-\", linewidth=2, markersize=3)\n",
    "axes[1].set_xlabel(\"Number of Clusters (k)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Silhouette Score\", fontsize=12)\n",
    "axes[1].set_title(\"Silhouette Score Method\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817fb088",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 4\n",
    "\n",
    "logging.debug(f\"Fitting K-Means with k={optimal_k}\")\n",
    "\n",
    "k_means_final = KMeans(n_clusters=optimal_k, random_state=random_state, n_init=10)\n",
    "cluster_labels = k_means_final.fit_predict(features_scaled)\n",
    "\n",
    "customer_features[\"cluster\"] = cluster_labels\n",
    "\n",
    "logging.info(f\"Final inertia: {k_means_final.inertia_:.2f}\")\n",
    "logging.info(f\"Silhouette Score: {silhouette_score(features_scaled, cluster_labels):.3f}\")\n",
    "\n",
    "cluster_counts = customer_features[\"cluster\"].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(customer_features)) * 100\n",
    "    logging.info(f\"Cluster {cluster_id}: {count} customers ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c6cb4",
   "metadata": {},
   "source": [
    "## Cluster Analysis and Profiling\n",
    "- 视频有点难看，代码很多，我懒得再抄了\n",
    "- 而且我们模型不会一样\n",
    "- 可以自己研究一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fe909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要先做好一个cluster_profiles\n",
    "# 让后这个是McKinsey他们的定义，只是参考\n",
    "\n",
    "def interpret_cluster(cluster_id, profile):\n",
    "    row = profile.loc[cluster_id]\n",
    "\n",
    "    high_freq = row[\"frequency_orders\"] > cluster_profiles[\"frequency_orders\"].median()\n",
    "    high_monetary = row[\"monetary_total\"] > cluster_profiles[\"monetary_total\"].median()\n",
    "    low_recency = row[\"recency_days\"] < cluster_profile[\"recency_days\"].median()\n",
    "    has_sub = row[\"has_subsciption\"] > 0.5\n",
    "    high_engagement = row[\"total_events\"] > cluster_profiles[\"total_events\"].median()\n",
    "\n",
    "    if high_freq and high_monetary and has_sub:\n",
    "        name = \"VIP Champions\"\n",
    "        description = \"High value loyal customers with active subscriptions\"\n",
    "    elif high_freq and low_recency:\n",
    "        name = \"Loyal Regulars\"\n",
    "        description = \"Frequent buyers who purchase regularly\"\n",
    "    elif not low_recency and not high_freq:\n",
    "        name = \"At-Risk / Hibernating\"\n",
    "        description = \"Previously active but haven't purchased recently\"\n",
    "    else:\n",
    "        name = \"Casual Explorers\"\n",
    "        description = \"Infrequent buyers, potential growth\"\n",
    "    return name, description\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    name, description = interpret_cluster(cluster_id, cluster_profiles)\n",
    "    cluster_names[cluster_id] = name\n",
    "    cluster_descriptions[cluster_id] = description\n",
    "\n",
    "    size = (customer_features[\"cluster\"] == cluster_id).sum()\n",
    "    percentage = (size / len(customer_features)) * 100\n",
    "\n",
    "    revenue_info = revenue_analysis(revenue_analysis[\"cluster\"] == cluster_id).iloc[0]\n",
    "    logging.info(f\"Cluster {cluster_id}: {name}\")\n",
    "    logging.info(f\"Description: {description}\")\n",
    "    logging.info(f\"Size: {size} customers ({percentage:.1f}%)\")\n",
    "    # logging.info(f\"Revenue contribution: {revenue_info[\"total_revenue\"]:.2f} ({revenue_info[\"total_pct\"]:.2f}% of total)\")\n",
    "\n",
    "\n",
    "    row = cluster_profiles.loc[cluster_id]\n",
    "    # log out avg recency, avg frequency, avg total spend, avg order value\n",
    "    # subscription rate, avg engagement events, voucher usage rate\n",
    "    # referral rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6bb29",
   "metadata": {},
   "source": [
    "## Business Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "\n",
    "for _, rev_row in revenue_analysis.iterrows():\n",
    "    cluster_id = int(rev_row[\"cluster\"])\n",
    "    row = cluster_profiles.loc[cluster_id]\n",
    "    name = cluster_names[cluster_id]\n",
    "    descriptions = cluster_descriptions[cluster_id]\n",
    "    size = int(rev_row[\"customer_count\"])\n",
    "\n",
    "    strategies = []\n",
    "\n",
    "    if row[\"monetary_total\"] > cluster_profiles[\"monetary_total\"].quantile(0.75):\n",
    "        strategies.append(\"VIP program: Exclusive early access to new products\")\n",
    "        strategies.append(\"Premium Offerings: Introduce limited edition or premium...\")\n",
    "    # something like this...\n",
    "\n",
    "\n",
    "# Also find the priority of each cluster and show why(size%, revenue contribution, avg revenue per customer)\n",
    "# sort by this priority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5add49",
   "metadata": {},
   "source": [
    "## The final customer features looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(customer_features.head())\n",
    "\n",
    "customer_features.to_csv(\"./customer_features.csv\", index=False)\n",
    "\n",
    "# 目前按照primary_currency划分\n",
    "print(\"primary_currency_x missing values:\", customer_features[\"primary_currency_x\"].isnull().sum())\n",
    "print(\"primary_currency_y missing values:\", customer_features[\"primary_currency_y\"].isnull().sum())\n",
    "print(\"customer_features shape:\", customer_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc94c4d",
   "metadata": {},
   "source": [
    "## Split into different countries\n",
    "Our group decides to split users into different countries for considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52660b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_user = customer_features[customer_features[\"primary_currency_x\"] == \"SGD\"]\n",
    "myr_user = customer_features[customer_features[\"primary_currency_x\"] == \"MYR\"]\n",
    "\n",
    "display(sgd_user.head())\n",
    "display(myr_user.head())\n",
    "print(\"sgd shape\", sgd_user.shape)\n",
    "print(\"myr shape\", myr_user.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94a8df",
   "metadata": {},
   "source": [
    "## hierarchical clustering (SGD users)\n",
    "We cannot dirctly apply k-means, it is useless. Now We tried to use hierarchical clustering to do the clustering.\n",
    "- The first layer: we split the customers into three groups: high value, mid value, low value. The criteria is the RFM.\n",
    "- The second layer: within each group, we do k-means clustering again based on other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d16afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "scaler = StandardScaler()\n",
    "\"\"\"\n",
    "first we select our features for clustering of the first layer, i choose the RFM features,\n",
    "and the avg_order_valueis caculated from monetary/ frequency\n",
    "\"\"\"\n",
    "value_cols = ['recency_days', 'frequency_orders', 'monetary_total']\n",
    "\n",
    "value_data = sgd_user[value_cols].copy()\n",
    "\n",
    "\n",
    "# remove the null values, is 0 acceptable? i am not sure.\n",
    "value_data = value_data.fillna(0)\n",
    "\n",
    "\n",
    "# standradize the data\n",
    "scaler_value = StandardScaler()\n",
    "value_data_scaled = scaler_value.fit_transform(value_data)\n",
    "\n",
    "\n",
    "\n",
    "# I consider for 3 clusters(later we can change to more), high, medium, low value customers\n",
    "kmeas_value = KMeans(n_clusters=3, random_state=42)\n",
    "sgd_user['value_cluster'] = kmeas_value.fit_predict(value_data_scaled)\n",
    "logging.info(sgd_user.groupby('value_cluster')[value_cols].mean())\n",
    "\n",
    "# calculate the silhouette score to see how well the clustering is done\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(value_data_scaled, sgd_user['value_cluster'])\n",
    "logging.info(f'Silhouette Score for Value Clustering: {score:.4f}')\n",
    "# this seems ok, because it is data from people, people are hard to cluster well\n",
    "\n",
    "print(sgd_user['value_cluster'].value_counts())\n",
    "# now maybe we consider to use the snake plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# elbow plot\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 8)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(value_data_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(value_data_scaled, labels))\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, marker='o', color = 'red')\n",
    "plt.title('Elbow method (RFM)')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_range, silhouette_scores, 'o-', color='blue')\n",
    "plt.title('Silhouette Score (RFM)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#################################################################\n",
    "plot_data= pd.DataFrame(value_data_scaled, columns=value_cols)\n",
    "plot_data['Cluster'] = sgd_user['value_cluster']\n",
    "\n",
    "plot_data_melted = pd.melt(plot_data,\n",
    "                           id_vars=['Cluster'],\n",
    "                           value_vars=value_cols,\n",
    "                           var_name='Metric',\n",
    "                           value_name='Value')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=plot_data_melted, x='Metric', y='Value', hue='Cluster', marker='o', palette='bright')\n",
    "plt.title('Snake Plot of RFM Clusters')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Standardized Value (z-score)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36ac0f",
   "metadata": {},
   "source": [
    "## Plot of r,f,,m of SGD users\n",
    "before we go to the hierarchical clustering, we plot the r,f,m of SGD users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f491f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "rfm_score_cols = ['recency_days', 'frequency_orders', 'monetary_total']\n",
    "\n",
    "plt.figure(figsize=(18, 5))  # 更大画布\n",
    "for i, col in enumerate(rfm_score_cols):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(sgd_user[col], bins=50, kde=False, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'{col} Distribution', fontsize=16)\n",
    "    plt.xlabel(col, fontsize=13)\n",
    "    plt.ylabel('User Count', fontsize=13)\n",
    "    plt.tick_params(axis='x', labelrotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf5d8b",
   "metadata": {},
   "source": [
    "## Continue Hierarchical Clustering (四分位)\n",
    "- We can also split the customers into four groups based on RFM features: top 25%, mid-high 25%, mid-low 25%, low 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d53e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 以下代码是给赋分制的聚类准备的函数\n",
    "def rfm_score_by_rank(series, reverse=False):\n",
    "    n = len(series)\n",
    "\n",
    "    # 排名,给值从小到大排\n",
    "    rank = series.rank(method=\"first\")  # 1,2,3,...,n\n",
    "\n",
    "    # 根据排名位置计算分数\n",
    "    score = pd.cut(\n",
    "        rank,\n",
    "        bins=[0, 0.25*n, 0.50*n, 0.75*n, n],\n",
    "        labels=[1, 2, 3, 4],\n",
    "        include_lowest=True\n",
    "    ).astype(int)\n",
    "\n",
    "    # recency 越小越好 → 反转分数\n",
    "    if reverse:\n",
    "        score = 5 - score\n",
    "\n",
    "    return score\n",
    "\n",
    "value_cols = ['recency_days', 'frequency_orders', 'monetary_total']\n",
    "rfm_raw = sgd_user[value_cols].copy()\n",
    "\n",
    "rfm_scored = pd.DataFrame({\n",
    "    'R_score': rfm_score_by_rank(rfm_raw['recency_days'], reverse=True),   # R 越小越好\n",
    "    'F_score': rfm_score_by_rank(rfm_raw['frequency_orders'], reverse=False),\n",
    "    'M_score': rfm_score_by_rank(rfm_raw['monetary_total'], reverse=False),\n",
    "}, index=sgd_user.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "value_data_scored = rfm_scored[['R_score', 'F_score', 'M_score']].astype(float).values\n",
    "\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 8)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(value_data_scored)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(value_data_scored, labels))\n",
    "\n",
    "# 画肘部图 & 轮廓系数\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, marker='o')\n",
    "plt.title('Elbow method (RFM quartile scores)')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score (RFM quartile scores)')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将这个变成新的features\n",
    "sgd_user = sgd_user.join(rfm_scored)\n",
    "display(sgd_user.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed76d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_final = 3   # 用 3 类\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42)\n",
    "sgd_user['value_cluster_quartile'] = kmeans_final.fit_predict(value_data_scored)\n",
    "\n",
    "print(sgd_user['value_cluster_quartile'].value_counts())\n",
    "\n",
    "logging.info(sgd_user.groupby('value_cluster_quartile')[value_cols].mean())\n",
    "\n",
    "# 查看基于赋分制的每簇平均 R/F/M 分\n",
    "cluster_profile = (\n",
    "    rfm_scored\n",
    "    .assign(Cluster=sgd_user['value_cluster_quartile'])\n",
    "    .groupby('Cluster')[['R_score', 'F_score', 'M_score']]\n",
    "    .mean()\n",
    ")\n",
    "print(cluster_profile)\n",
    "\n",
    "\n",
    "# 用赋分制 (1~4) 画蛇形图\n",
    "\n",
    "\n",
    "rfm_scored_with_cluster = rfm_scored.copy()\n",
    "rfm_scored_with_cluster['Cluster'] = sgd_user['value_cluster_quartile']\n",
    "\n",
    "plot_data = pd.melt(\n",
    "    rfm_scored_with_cluster,\n",
    "    id_vars=['Cluster'],\n",
    "    value_vars=['R_score', 'F_score', 'M_score'],\n",
    "    var_name='Metric',\n",
    "    value_name='Score'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=plot_data,\n",
    "    x='Metric',\n",
    "    y='Score',\n",
    "    hue='Cluster',\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "plt.title('Snake Plot of RFM Clusters (Rank-based Scores 1–4)')\n",
    "plt.xlabel('RFM Metrics')\n",
    "plt.ylabel('Average RFM Score')\n",
    "plt.ylim(1, 4)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa5913",
   "metadata": {},
   "source": [
    "下面文字是我画图以后让AI总结的，不一定对，我觉得recency_days倒是能精准给用户画像,SGD用户画像似乎没啥变换，还是这三类\n",
    "\n",
    "* **Cluster 0 (Blue Line) - Active Budget-Conscious Shoppers:**\n",
    "    * **Persona:** These customers are currently active (low Recency) but maintain below-average spending habits with a low Average Order Value (AOV).\n",
    "    * **Role:** They represent your **reliable base**, providing consistent but lower-value transactions.  是不是基本盘存疑，我觉得vip才是基本盘（绿线）\n",
    "    * **Strategy:** Focus on **Upselling** strategies like \"buy more, save more\" bundles or minimum spend thresholds for free shipping to incrementally increase their basket size.\n",
    "\n",
    "* **Cluster 1 (Orange Line) - Dormant High-Potential Users:**\n",
    "    * **Persona:** These customers have stopped purchasing (high Recency) but previously demonstrated a willingness to spend more per order (higher AOV) than the active mass group.\n",
    "    * **Role:** They are **high-value churned users** who previously showed quality purchasing power but have since disengaged.\n",
    "    * **Strategy:** Implement aggressive **Reactivation** campaigns using high-value incentives or premium product offers to win back their higher spending potential.\n",
    "\n",
    "* **Cluster 2 (Green Line) - Loyal VIP Spenders:**\n",
    "    * **Persona:** These are your top-tier customers, exhibiting exceptional frequency and total spending (very high Frequency and Monetary scores) while remaining highly active (very low Recency).\n",
    "    * **Role:** They are your **VIP core**, driving the majority of your revenue and engagement.\n",
    "    * **Strategy:** Prioritize **Retention** through exclusive VIP perks, early access to new products, and personalized appreciation rewards to maintain their loyalty and prevent churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472007e2",
   "metadata": {},
   "source": [
    "## Continue to Hierarchical Clustering (SGD)\n",
    "- Now we need to realize the second layer. As we can see the green line is the VIP group, se we need to focus on them, because they will bring most of the revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tea pod missing values:\", sgd_user[\"tea_pod_pct\"].isnull().sum())\n",
    "print(\"tea bag missing values:\", sgd_user[\"tea_bag_pct\"].isnull().sum())\n",
    "print(\"merchandise missing values:\", sgd_user[\"merchandise_pct\"].isnull().sum())\n",
    "print(\"bundle missing values:\", sgd_user[\"bundle_pct\"].isnull().sum())\n",
    "print(\"unique products missing values:\", sgd_user[\"unique_products\"].isnull().sum())\n",
    "#这个unique products缺失值太多  存疑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_df = sgd_user[sgd_user['value_cluster'] == 2]\n",
    "print(f\"there are {len(vip_df)} VIP customers identified.\")\n",
    "\n",
    "behavior_cols = [\n",
    "    'tea_pod_pct',         \n",
    "    'tea_bag_pct',\n",
    "    'merchandise_pct',\n",
    "    'bundle_pct',         \n",
    "    #'unque_products'\n",
    "]\n",
    "\n",
    "#X_vip = vip_df[behavior_cols].fillna(0)\n",
    "X_vip = vip_df[behavior_cols].copy()\n",
    "\n",
    "scaler_vip = StandardScaler()\n",
    "X_vip_scaled = scaler_vip.fit_transform(X_vip)\n",
    "\n",
    "k_vip = 4  # we can aslo try 3 maybe\n",
    "kmeans_vip = KMeans(n_clusters=k_vip, random_state=42)\n",
    "vip_df = vip_df.copy()\n",
    "vip_df['behavior_cluster'] = kmeans_vip.fit_predict(X_vip_scaled)\n",
    "\n",
    "\n",
    "vip_result = vip_df.groupby('behavior_cluster')[behavior_cols].mean()\n",
    "display(vip_result)\n",
    "\n",
    "score_vip = silhouette_score(X_vip_scaled, vip_df['behavior_cluster'])\n",
    "print(f'Silhouette Score for VIP Behavioral Clustering: {score_vip:.4f}')\n",
    "\n",
    "\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "k_range = range(2, 8)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_vip_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_vip_scaled, labels))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(list(k_range), inertias, marker='o', color='red')\n",
    "plt.title('Elbow Method (VIP)')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(list(k_range), silhouettes, marker='o', color='blue')\n",
    "plt.title('Silhouette Score (VIP)')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###########################################################\n",
    "plot_data_vip= pd.DataFrame(X_vip_scaled, columns=behavior_cols)\n",
    "plot_data_vip['Cluster'] = vip_df['behavior_cluster'].values\n",
    "plot_data_vip_melted = pd.melt(plot_data_vip,\n",
    "                           id_vars=['Cluster'],\n",
    "                           value_vars=behavior_cols,\n",
    "                           var_name='Metric',\n",
    "                           value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=plot_data_vip_melted, x='Metric', y='Value', hue='Cluster', marker='o', palette='bright')\n",
    "plt.title('Snake Plot of VIP Behavioral Clusters')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Standardized Value (z-score)')\n",
    "plt.xticks(rotation=30, ha='right', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "print(vip_df['behavior_cluster'].value_counts())    # the final group only has 3 people, so i think we should set 3.\n",
    "\n",
    "print(vip_df['behavior_cluster'].unique())\n",
    "print(vip_df['behavior_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2ed06",
   "metadata": {},
   "source": [
    "- 均值类用户（蓝线）：啥都买，这类用户可以给他们不同的套餐组合\n",
    "- 偏好类用户（橙线）：主要买茶胶囊不买茶包，这类用户给他们推荐茶胶囊的套餐组合\n",
    "- 周边用户（绿线）：主要买周边，这类用户可以给他们推荐新的周边产品，比如杯子等\n",
    "- 组合包用户（红线）：主要买组合包，这类应该每逢过节啥的推组合包"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f2272",
   "metadata": {},
   "source": [
    "## The funtion for hierarchical clustering (cluster 2)\n",
    "Because we have many clusters to do in the layer 2, so we define a function to do the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_behavior_k(df, behavior_cols, k_range=range(2, 8), title_prefix=\"Behavior\"):\n",
    "    \"\"\"\n",
    "    给定行为特征，扫描不同 k，画肘部图和轮廓系数图。\n",
    "    不修改 df，只返回标准化后的 X 和 scaler。\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 分离布尔列和非布尔列\n",
    "    bool_cols = [col for col in behavior_cols if df[col].dtype == bool]\n",
    "    num_cols = [col for col in behavior_cols if col not in bool_cols]\n",
    "\n",
    "    # 标准化非布尔列\n",
    "    X_num = df[num_cols].copy().fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = scaler.fit_transform(X_num) if len(num_cols) > 0 else np.empty((len(df), 0))\n",
    "\n",
    "    # 取布尔列（不做标准化，直接转为int型）\n",
    "    X_bool = df[bool_cols].copy().fillna(False).astype(int) if len(bool_cols) > 0 else pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 拼接\n",
    "    if len(bool_cols) > 0 and len(num_cols) > 0:\n",
    "        X_scaled = np.concatenate([X_num_scaled, X_bool.values], axis=1)\n",
    "        all_cols = num_cols + bool_cols\n",
    "    elif len(num_cols) > 0:\n",
    "        X_scaled = X_num_scaled\n",
    "        all_cols = num_cols\n",
    "    else:\n",
    "        X_scaled = X_bool.values\n",
    "        all_cols = bool_cols\n",
    "    \n",
    "    inertias = []\n",
    "    silhouettes = []\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "    # 画图\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(k_range), inertias, marker='o')\n",
    "    plt.title(f'Elbow Method ({title_prefix})')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(list(k_range), silhouettes, marker='o')\n",
    "    plt.title(f'Silhouette Score ({title_prefix})')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return X_scaled, scaler, inertias, silhouettes, all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22610642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_behavior(\n",
    "    df,\n",
    "    behavior_cols,\n",
    "    X_scaled=None,\n",
    "    scaler=None,\n",
    "    k_final=3,\n",
    "    cluster_col_name=\"behavior_cluster\",\n",
    "    title_prefix=\"Behavior\",\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    在选定的 k 下做最终聚类，并画蛇形图。\n",
    "    如果没传 X_scaled/scaler，会在内部重新做一次标准化。\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # 如果没有从外面传 X_scaled，就自己算一次\n",
    "    if X_scaled is None:\n",
    "        bool_cols = [col for col in behavior_cols if df_out[col].dtype == bool]\n",
    "        num_cols = [col for col in behavior_cols if col not in bool_cols]\n",
    "\n",
    "        X_num = df_out[num_cols].copy().fillna(0)\n",
    "        scaler = StandardScaler()\n",
    "        X_num_scaled = scaler.fit_transform(X_num) if len(num_cols) > 0 else np.empty((len(df_out), 0))\n",
    "        X_bool = df_out[bool_cols].copy().fillna(False).astype(int) if len(bool_cols) > 0 else pd.DataFrame(index=df_out.index)\n",
    "\n",
    "        if len(bool_cols) > 0 and len(num_cols) > 0:\n",
    "            X_scaled = np.concatenate([X_num_scaled, X_bool.values], axis=1)\n",
    "            all_cols = num_cols + bool_cols\n",
    "        elif len(num_cols) > 0:\n",
    "            X_scaled = X_num_scaled\n",
    "            all_cols = num_cols\n",
    "        else:\n",
    "            X_scaled = X_bool.values\n",
    "            all_cols = bool_cols\n",
    "    else:\n",
    "        all_cols = behavior_cols\n",
    "\n",
    "    # 最终聚类\n",
    "    kmeans = KMeans(n_clusters=k_final, random_state=42)\n",
    "    df_out[cluster_col_name] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # 每个簇的行为均值\n",
    "    cluster_means = df_out.groupby(cluster_col_name)[behavior_cols].mean()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nCluster means:\")\n",
    "        display(cluster_means)\n",
    "        print(\"\\nValue counts:\")\n",
    "        print(df_out[cluster_col_name].value_counts())\n",
    "\n",
    "    # 蛇形图：用 standardized 行为特征\n",
    "    plot_data = pd.DataFrame(X_scaled, columns=behavior_cols)\n",
    "    plot_data['Cluster'] = df_out[cluster_col_name].values\n",
    "\n",
    "    plot_data_melted = plot_data.melt(\n",
    "        id_vars=['Cluster'],\n",
    "        value_vars=behavior_cols,\n",
    "        var_name='Metric',\n",
    "        value_name='Value'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(\n",
    "        data=plot_data_melted,\n",
    "        x='Metric',\n",
    "        y='Value',\n",
    "        hue='Cluster',\n",
    "        marker='o'\n",
    "    )\n",
    "    plt.title(f'Snake Plot of {title_prefix} Clusters')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Standardized Value (z-score)')\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "    return df_out, kmeans, scaler, cluster_means, all_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253d57a",
   "metadata": {},
   "source": [
    "## Countinue Hierarchical Clustering (cluster 2, on price)\n",
    "Now we use following features to do the clustering:\n",
    "- 'monetary_total',\n",
    "- 'avg_order_value',\n",
    "- 'discount_saving_ratio',\n",
    "- 'voucher_usage_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_df = sgd_user[sgd_user['value_cluster'] == 2]\n",
    "print(f\"VIP numbers: {len(vip_df)}\")\n",
    "\n",
    "behavior_cols = [\n",
    "    'monetary_total',\n",
    "    'avg_order_value',\n",
    "    'discount_savings_ratio',\n",
    "    'voucher_usage_rate',\n",
    "]\n",
    "\n",
    "X_vip_scaled, vip_scaler, inertias, silhouettes = evaluate_behavior_k(\n",
    "    vip_df,\n",
    "    behavior_cols,\n",
    "    k_range=range(2, 8),\n",
    "    title_prefix=\"VIP Behavior\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_df_clustered, kmeans_vip, vip_scaler, vip_means = cluster_behavior(\n",
    "    vip_df,\n",
    "    behavior_cols,\n",
    "    X_scaled=X_vip_scaled, \n",
    "    scaler=vip_scaler,\n",
    "    k_final=3,\n",
    "    cluster_col_name=\"behavior_cluster\",\n",
    "    title_prefix=\"VIP Behavior\",\n",
    "    verbose=True\n",
    ")\n",
    "#如果想填回主表  做这一行\n",
    "#sgd_user.loc[vip_df_clustered.index, 'behavior_cluster'] = vip_df_clustered['behavior_cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02aee98",
   "metadata": {},
   "source": [
    "- 黑线：这类VIP用户，消费总量很多，但是不爱用各种折扣和优惠券，说明他们对价格不敏感，更看重品质和服务，可以考虑给他们提供更高端的产品和个性化的服务。\n",
    "- 粉线：这类VIP用户尤其爱优惠券和折扣，并且需要注意的是他们平均订单金额也很高，说明他们是精打细算型的高消费用户，可以考虑给他们提供更多的优惠券和折扣活动，吸引他们继续购买。\n",
    "- 橙线：这类VIP用户是佛系用户，对折扣不敏感，并且消费总额和平均订单金额也不是很高，说明是一种大众的VIP用户，不知道咋提高他们的消费欲望。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0795b7",
   "metadata": {},
   "source": [
    "## Continue Hierarchical Clustering (cluster 2, 口味偏好)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_intensity_df = customer_features[customer_features['intensity'] != 'unknown']\n",
    "print(f\"{users_with_intensity_df.shape}\")\n",
    "\n",
    "display(users_with_intensity_df['intensity'].value_counts())\n",
    "\n",
    "vip_df = sgd_user[sgd_user['value_cluster'] == 2]\n",
    "\n",
    "vip_with_taste_df = vip_df.merge(\n",
    "    users_with_intensity_df[['user_id', 'intensity']],\n",
    "    on='user_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"vip 且有口味偏好的用户有多少个: {len(vip_with_taste_df)}\")\n",
    "display(vip_with_taste_df['intensity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343b75c",
   "metadata": {},
   "source": [
    "所以上面那个做不了聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01601bf",
   "metadata": {},
   "source": [
    "## Continue Hierarchical Clustering (cluster 2, on social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ca6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_user['is_referred_user'] = sgd_user['is_referred_user'].astype(bool)\n",
    "display(sgd_user['is_referred_user'].value_counts())\n",
    "\n",
    "vip_df = sgd_user[sgd_user['value_cluster'] == 2]\n",
    "\n",
    "behavior_cols = [\n",
    "    'is_referred_user',\n",
    "    'num_referred_users',\n",
    "]\n",
    "\n",
    "X_vip_social_scaled, vip_social_scaler, inertias, silhouettes = evaluate_behavior_k(\n",
    "    vip_df,\n",
    "    behavior_cols,\n",
    "    k_range=range(2, 8),\n",
    "    title_prefix=\"VIP Social Behavior\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_df_social_clustered, kmeans_vip_social, vip_social_scaler, vip_social_means = cluster_behavior(\n",
    "    vip_df,\n",
    "    behavior_cols,\n",
    "    X_scaled=X_vip_social_scaled, \n",
    "    scaler=vip_social_scaler,\n",
    "    k_final=3,\n",
    "    cluster_col_name=\"social_cluster\",\n",
    "    title_prefix=\"VIP Social Behavior\",\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
